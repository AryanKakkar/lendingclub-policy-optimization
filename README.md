# ğŸ’° LendingClub Policy Optimization
*A comparative study of supervised default prediction and offline reinforcement learning for credit approval.*

---

## ğŸ§© Project Structure

```text
lendingclub-policy-optimization/
â”‚
â”œâ”€â”€ notebooks/
â”‚   â”œâ”€â”€ 01_eda.ipynb              # Data exploration and visualization
â”‚   â”œâ”€â”€ 02_supervised_model.ipynb # Deep learning classifier (AUC/F1)
â”‚   â””â”€â”€ 03_offline_rl.ipynb       # Conservative Q-Learning (CQL) + FQE evaluation
â”‚
â”œâ”€â”€ src/
â”‚   â”œâ”€â”€ data_utils.py             # Data loading, cleaning, splitting
â”‚   â”œâ”€â”€ features.py               # Preprocessing & transformers
â”‚   â”œâ”€â”€ profit_threshold.py       # Reward functions and threshold logic
â”‚   â””â”€â”€ rl_cql.py                 # RL agent, MDP dataset, and evaluation
â”‚
â”œâ”€â”€ requirements.txt
â”œâ”€â”€ README.md
â””â”€â”€ reports/
    â””â”€â”€ LendingClub_Policy_Optimization_Report.pdf


---

## âš™ï¸ Environment Setup
### 1ï¸âƒ£ Create a Virtual Environment
```bash
python3 -m venv .venv
source .venv/bin/activate    # (Mac/Linux)
````

### 2ï¸âƒ£ Install Requirements

```bash
pip install -r requirements.txt
```

Main dependencies:

* `pandas`, `numpy`, `scikit-learn`
* `matplotlib`, `seaborn`
* `d3rlpy==2.x`, `joblib`, `pathlib`

---

## ğŸ§  Running the Notebooks

1. **EDA:** Explore & preprocess LendingClub dataset (`01_eda.ipynb`).
2. **Supervised Model:** Train classifier, compute AUC/F1 (`02_supervised_model.ipynb`).
3. **Offline RL:** Build MDP dataset, train CQL agent, estimate policy value (`03_offline_rl.ipynb`).

All relative paths are consistent; update `DATA_PATH` if CSV is elsewhere.

---

## ğŸ“Š Key Results

| Metric                     | Model          | Value                 |
| -------------------------- | -------------- | --------------------- |
| **AUC**                    | Deep Learning  | ~0.83                 |
| **F1-Score**               | Deep Learning  | ~0.71                 |
| **Estimated Policy Value** | RL Agent (CQL) | â‰ˆ â€“3.3 Ã— 10Â³ per loan |

---

## ğŸ§© Insights

* DL model â†’ accurate default risk ranking.
* RL agent â†’ directly optimizes for profit but mimics approve-all policy due to missing denied samples.
* Reward design and action coverage are crucial for successful offline RL.

---

## ğŸš€ Future Work

* Collect denied-loan data for counterfactual learning.
* Improve reward realism (recovery, costs).
* Test newer offline RL algorithms (IQL, BCQ).
* Combine DL scoring + RL policy layers.

```



